{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(y):\n",
    "    n_classes = np.unique(y)\n",
    "    one_hot = np.zeros((len(y), len(n_classes)))\n",
    "    for i, c in enumerate(y):\n",
    "        one_hot[i, n_classes == c] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Assuming you have four classes represented by numbers 0 to 3\n",
    "y = np.array([0, 1, 2, 3, 0, 2, 1, 3, 0])\n",
    "\n",
    "# Convert classes to one-hot encoding\n",
    "one_hot_y = one_hot(y)\n",
    "\n",
    "print(one_hot_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z)\n",
    "    sum_exps = np.sum(exps)\n",
    "    return exps / sum_exps\n",
    "\n",
    "z = np.array([1.0, 2.0, 3.0])\n",
    "probabilities = softmax(z)\n",
    "\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logestic Regression Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.00001, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot(y):\n",
    "        n_classes = np.unique(y)\n",
    "        one_hot = np.zeros((len(y), len(n_classes)))\n",
    "        for i, c in enumerate(y):\n",
    "            one_hot[i, n_classes == c] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def probabilities(self, X):\n",
    "        z = np.dot(X, self.w.T)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return np.argmax(self.probabilities(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1) # (samples,dim) -> (samples, dim + 1)\n",
    "        self.w = np.zeros((len(np.unique(y)), X.shape[1])) # (4, 301)\n",
    "        y = self.one_hot(y)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            predictions = self.probabilities(X)\n",
    "            error = predictions - y\n",
    "            gradient = np.dot(error.T, X)\n",
    "            self.w -= self.lr * gradient\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data\n",
    "np.random.seed(0) # For consistent results\n",
    "X = np.random.rand(100, 300) # 100 samples, 300 features each\n",
    "y = np.random.choice([0, 1, 2, 3], 100) # 100 targets belonging to one of four classes\n",
    "\n",
    "\n",
    "# Creating an instance of the Logistic Regression class\n",
    "model = LogisticRegression(lr=0.00001, n_iter=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after adding bias: (100, 301)\n",
      "Initial shape of weights: (4, 301)\n",
      "Shape of y after one-hot encoding: (100, 4)\n",
      "Shape of predictions: (100, 4)\n",
      "Shape of error: (100, 4)\n",
      "Shape of gradient: (4, 301)\n",
      "Shape of weights after update: (4, 301)\n"
     ]
    }
   ],
   "source": [
    "# Adding bias to the feature matrix\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "print(f'Shape of X after adding bias: {X.shape}')  # (100, 301)\n",
    "\n",
    "# Initializing weights\n",
    "model.w = np.zeros((len(np.unique(y)), X.shape[1]))\n",
    "print(f'Initial shape of weights: {model.w.shape}')  # (4, 301)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y = model.one_hot(y)\n",
    "print(f'Shape of y after one-hot encoding: {y.shape}')  # (100, 4)\n",
    "\n",
    "for _ in range(model.n_iter):\n",
    "    # Compute probabilities\n",
    "    predictions = model.probabilities(X)\n",
    "    print(f'Shape of predictions: {predictions.shape}')  # (100, 4)\n",
    "\n",
    "    # Compute error\n",
    "    error = predictions - y\n",
    "    print(f'Shape of error: {error.shape}')  # (100, 4)\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = np.dot(error.T, X)\n",
    "    print(f'Shape of gradient: {gradient.shape}')  # (4, 301)\n",
    "\n",
    "    # Update weights\n",
    "    model.w -= model.lr * gradient\n",
    "    print(f'Shape of weights after update: {model.w.shape}')  # (4, 301)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
